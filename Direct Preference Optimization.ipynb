{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization (DPO)\n",
    "\n",
    "Direct Preference Optimziation is an approach to improve the alignment of a language model. Current approaches for training large language models utilize an intial step of unsupervised training on a large corpus of data. This results in a model that generates language most likely to follows some prompt given the data that the model is trained on. When we want out language model to be used as a chat bot or code assistant, this oftentimes produces undesirable text. High quality conversations or coding examples may be rare in our training corpus, thus rare in our model's output.\n",
    "\n",
    "To address this, we can utilize a dataset containing three columns: a prompt, a chosen output, and a rejected output.\n",
    "\n",
    "It is an alternative to reinforcement learning from human feedback (RLHF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import DPOTrainer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# note: to get bitsandbytes to work on windows, uninstall bitsandbytes and reinstall with\n",
    "# pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\Users\\danto\\anaconda3\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "def process_hh_rlhf_sample(sample):\n",
    "    \"\"\"\n",
    "    sample is a dictionary with keys 'chosen' and 'rejected'.\n",
    "    Extract the prompt and the two completions from the sample.\n",
    "    Find the index of the last substring '\\n\\nAssistant:\n",
    "\n",
    "    Return a dictionary with keys prompt, chosen, and rejected.\n",
    "    \"\"\"\n",
    "    term = '\\n\\nAssistant: '\n",
    "    end_of_prompt_index = sample['chosen'].rfind(term)\n",
    "\n",
    "    # extract the prompt\n",
    "    prompt = sample['chosen'][:end_of_prompt_index+len(term)]\n",
    "    # extract the chosen completion\n",
    "    chosen = sample['chosen'][len(prompt):]\n",
    "    # extract the rejected completion\n",
    "    rejected = sample['rejected'][len(prompt):]\n",
    "\n",
    "    return {'prompt': prompt, 'chosen': chosen, 'rejected': rejected}\n",
    "\n",
    "def get_anthropic_hh_rlhf_dataset(split='train'):\n",
    "    \"\"\"\n",
    "    The Anthropic HH-RLHF dataset contains 160k training examples and 8k test examples.\n",
    "    Each example is a dictionary with two keys: 'chosen' and 'rejected'.\n",
    "    Each of these includes the prompt and the completion.\n",
    "    I want to extract the prompt, chosen completion, and rejected completion.\n",
    "\n",
    "    https://arxiv.org/abs/2204.05862\n",
    "    https://huggingface.co/datasets/Anthropic/hh-rlhf\n",
    "    \"\"\"\n",
    "    dataset = load_dataset('Anthropic/hh-rlhf', split=split)\n",
    "    return dataset.map(process_hh_rlhf_sample)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'stabilityai/stablelm-2-1_6b',\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'stabilityai/stablelm-2-1_6b',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# https://github.com/huggingface/trl/issues/1073\n",
    "tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n",
    "tokenizer.bos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "train_dataset = get_anthropic_hh_rlhf_dataset(split='train')\n",
    "test_dataset = get_anthropic_hh_rlhf_dataset(split='test[:1000]') # use a small test set for now\n",
    "\n",
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=64, # only 64 gradient updates, not even one epoch\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    output_dir='output',\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='logs',\n",
    "    logging_steps=16,\n",
    "    lr_scheduler_type='constant' # default is linear\n",
    ")\n",
    "\n",
    "# after training, I can view the logs with:\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64, # dimension of the low-rank matrices\n",
    "    lora_alpha=16, # scaling factor for the weight matrices\n",
    "    bias='none', # don't train bias params\n",
    "    task_type='CASUAL_LM',\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'o_proj',\n",
    "        'gate_proj',\n",
    "        'up_proj',\n",
    "        'down_proj',\n",
    "        'lm_head',\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danto\\anaconda3\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:72: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples['prompt'], examples['chosen'], examples['rejected'], padding=True, truncation=True)\n",
    "\n",
    "encoded_dataset_train = train_dataset.map(tokenize_func, batched=True)\n",
    "encoded_dataset_test = test_dataset.map(tokenize_func, batched=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    beta=0.1, # beta param for DPO loss\n",
    "    train_dataset=encoded_dataset_train,\n",
    "    eval_dataset=encoded_dataset_test,\n",
    "    max_length=512,\n",
    "    max_target_length=128,\n",
    "    max_prompt_length=128,\n",
    "    generate_during_eval=False,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danto\\anaconda3\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 125/125 [13:36<00:00,  6.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6931473612785339, 'eval_runtime': 818.5613, 'eval_samples_per_second': 1.222, 'eval_steps_per_second': 0.153, 'eval_rewards/chosen': 0.0, 'eval_rewards/rejected': 0.0, 'eval_rewards/accuracies': 0.0, 'eval_rewards/margins': 0.0, 'eval_logps/rejected': -108.19891357421875, 'eval_logps/chosen': -87.72126770019531, 'eval_logits/rejected': -1.9339709281921387, 'eval_logits/chosen': -2.053931951522827}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model before training\n",
    "print(trainer.evaluate()) # evaluation will take about 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 16/64 [33:08<3:01:16, 226.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0057, 'learning_rate': 0.001, 'rewards/chosen': -1.0836848020553589, 'rewards/rejected': -0.8774189949035645, 'rewards/accuracies': 0.40625, 'rewards/margins': -0.2062658816576004, 'logps/rejected': -164.83995056152344, 'logps/chosen': -152.500244140625, 'logits/rejected': -2.065371036529541, 'logits/chosen': -2.0074422359466553, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 32/64 [56:36<47:22, 88.81s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8486, 'learning_rate': 0.001, 'rewards/chosen': -1.3901917934417725, 'rewards/rejected': -1.674965739250183, 'rewards/accuracies': 0.578125, 'rewards/margins': 0.2847740054130554, 'logps/rejected': -139.48309326171875, 'logps/chosen': -131.59625244140625, 'logits/rejected': -2.0241663455963135, 'logits/chosen': -1.993961215019226, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 48/64 [2:32:48<1:06:29, 249.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9214, 'learning_rate': 0.001, 'rewards/chosen': -1.5270169973373413, 'rewards/rejected': -1.4379708766937256, 'rewards/accuracies': 0.5, 'rewards/margins': -0.08904620260000229, 'logps/rejected': -131.55795288085938, 'logps/chosen': -154.04161071777344, 'logits/rejected': -2.503689765930176, 'logits/chosen': -2.3918697834014893, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [3:03:35<00:00, 172.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0459, 'learning_rate': 0.001, 'rewards/chosen': -2.09584379196167, 'rewards/rejected': -2.092801809310913, 'rewards/accuracies': 0.46875, 'rewards/margins': -0.003042057156562805, 'logps/rejected': -148.13485717773438, 'logps/chosen': -136.04318237304688, 'logits/rejected': -2.208425760269165, 'logits/chosen': -2.2577285766601562, 'epoch': 0.0}\n",
      "{'train_runtime': 11015.4092, 'train_samples_per_second': 0.023, 'train_steps_per_second': 0.006, 'train_loss': 0.9553848505020142, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=64, training_loss=0.9553848505020142, metrics={'train_runtime': 11015.4092, 'train_samples_per_second': 0.023, 'train_steps_per_second': 0.006, 'train_loss': 0.9553848505020142, 'epoch': 0.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rewards/chosen is the mean difference between the log probabilities of the policy model and \n",
    "# the reference model for the chosen completion, scaled by beta.\n",
    "# rewards/rejected is the mean difference between the log probabilities of the policy model and \n",
    "# the reference model for the rejected completion, scaled by beta.\n",
    "# rewards/accuracy is the mean of how often the chosen reward is greater than the corresponding rejected reward.\n",
    "# rewards/margins is the mean difference between the chosen and corresponding rejected rewards\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [04:01<00:00,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1580718755722046, 'eval_runtime': 243.056, 'eval_samples_per_second': 4.114, 'eval_steps_per_second': 0.514, 'eval_rewards/chosen': -3.0772223472595215, 'eval_rewards/rejected': -3.5384018421173096, 'eval_rewards/accuracies': 0.5139999985694885, 'eval_rewards/margins': 0.46117931604385376, 'eval_logps/rejected': -143.58291625976562, 'eval_logps/chosen': -118.49349975585938, 'eval_logits/rejected': -2.3071069717407227, 'eval_logits/chosen': -2.416668653488159, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('out')\n",
    "eval_results = trainer.evaluate() # evaluation will take about 30 minutes\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.0057,\n",
       "  'learning_rate': 0.001,\n",
       "  'rewards/chosen': -1.0836848020553589,\n",
       "  'rewards/rejected': -0.8774189949035645,\n",
       "  'rewards/accuracies': 0.40625,\n",
       "  'rewards/margins': -0.2062658816576004,\n",
       "  'logps/rejected': -164.83995056152344,\n",
       "  'logps/chosen': -152.500244140625,\n",
       "  'logits/rejected': -2.065371036529541,\n",
       "  'logits/chosen': -2.0074422359466553,\n",
       "  'epoch': 0.0,\n",
       "  'step': 16},\n",
       " {'loss': 0.8486,\n",
       "  'learning_rate': 0.001,\n",
       "  'rewards/chosen': -1.3901917934417725,\n",
       "  'rewards/rejected': -1.674965739250183,\n",
       "  'rewards/accuracies': 0.578125,\n",
       "  'rewards/margins': 0.2847740054130554,\n",
       "  'logps/rejected': -139.48309326171875,\n",
       "  'logps/chosen': -131.59625244140625,\n",
       "  'logits/rejected': -2.0241663455963135,\n",
       "  'logits/chosen': -1.993961215019226,\n",
       "  'epoch': 0.0,\n",
       "  'step': 32},\n",
       " {'loss': 0.9214,\n",
       "  'learning_rate': 0.001,\n",
       "  'rewards/chosen': -1.5270169973373413,\n",
       "  'rewards/rejected': -1.4379708766937256,\n",
       "  'rewards/accuracies': 0.5,\n",
       "  'rewards/margins': -0.08904620260000229,\n",
       "  'logps/rejected': -131.55795288085938,\n",
       "  'logps/chosen': -154.04161071777344,\n",
       "  'logits/rejected': -2.503689765930176,\n",
       "  'logits/chosen': -2.3918697834014893,\n",
       "  'epoch': 0.0,\n",
       "  'step': 48},\n",
       " {'loss': 1.0459,\n",
       "  'learning_rate': 0.001,\n",
       "  'rewards/chosen': -2.09584379196167,\n",
       "  'rewards/rejected': -2.092801809310913,\n",
       "  'rewards/accuracies': 0.46875,\n",
       "  'rewards/margins': -0.003042057156562805,\n",
       "  'logps/rejected': -148.13485717773438,\n",
       "  'logps/chosen': -136.04318237304688,\n",
       "  'logits/rejected': -2.208425760269165,\n",
       "  'logits/chosen': -2.2577285766601562,\n",
       "  'epoch': 0.0,\n",
       "  'step': 64},\n",
       " {'train_runtime': 11015.4092,\n",
       "  'train_samples_per_second': 0.023,\n",
       "  'train_steps_per_second': 0.006,\n",
       "  'total_flos': 2277255293411328.0,\n",
       "  'train_loss': 0.9553848505020142,\n",
       "  'epoch': 0.0,\n",
       "  'step': 64},\n",
       " {'eval_loss': 1.1580718755722046,\n",
       "  'eval_runtime': 299.9529,\n",
       "  'eval_samples_per_second': 3.334,\n",
       "  'eval_steps_per_second': 0.417,\n",
       "  'eval_rewards/chosen': -3.0772223472595215,\n",
       "  'eval_rewards/rejected': -3.5384018421173096,\n",
       "  'eval_rewards/accuracies': 0.5139999985694885,\n",
       "  'eval_rewards/margins': 0.46117931604385376,\n",
       "  'eval_logps/rejected': -143.58291625976562,\n",
       "  'eval_logps/chosen': -118.49349975585938,\n",
       "  'eval_logits/rejected': -2.3071069717407227,\n",
       "  'eval_logits/chosen': -2.416668653488159,\n",
       "  'epoch': 0.0,\n",
       "  'step': 64},\n",
       " {'eval_loss': 1.1580718755722046,\n",
       "  'eval_runtime': 243.056,\n",
       "  'eval_samples_per_second': 4.114,\n",
       "  'eval_steps_per_second': 0.514,\n",
       "  'eval_rewards/chosen': -3.0772223472595215,\n",
       "  'eval_rewards/rejected': -3.5384018421173096,\n",
       "  'eval_rewards/accuracies': 0.5139999985694885,\n",
       "  'eval_rewards/margins': 0.46117931604385376,\n",
       "  'eval_logps/rejected': -143.58291625976562,\n",
       "  'eval_logps/chosen': -118.49349975585938,\n",
       "  'eval_logits/rejected': -2.3071069717407227,\n",
       "  'eval_logits/chosen': -2.416668653488159,\n",
       "  'epoch': 0.0,\n",
       "  'step': 64}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to view the training loss after each step, use:\n",
    "\n",
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>rewards/chosen</th>\n",
       "      <th>rewards/rejected</th>\n",
       "      <th>rewards/accuracies</th>\n",
       "      <th>rewards/margins</th>\n",
       "      <th>logps/rejected</th>\n",
       "      <th>logps/chosen</th>\n",
       "      <th>logits/rejected</th>\n",
       "      <th>logits/chosen</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>eval_rewards/chosen</th>\n",
       "      <th>eval_rewards/rejected</th>\n",
       "      <th>eval_rewards/accuracies</th>\n",
       "      <th>eval_rewards/margins</th>\n",
       "      <th>eval_logps/rejected</th>\n",
       "      <th>eval_logps/chosen</th>\n",
       "      <th>eval_logits/rejected</th>\n",
       "      <th>eval_logits/chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0057</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-1.083685</td>\n",
       "      <td>-0.877419</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>-0.206266</td>\n",
       "      <td>-164.839951</td>\n",
       "      <td>-152.500244</td>\n",
       "      <td>-2.065371</td>\n",
       "      <td>-2.007442</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8486</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-1.390192</td>\n",
       "      <td>-1.674966</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>0.284774</td>\n",
       "      <td>-139.483093</td>\n",
       "      <td>-131.596252</td>\n",
       "      <td>-2.024166</td>\n",
       "      <td>-1.993961</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9214</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-1.527017</td>\n",
       "      <td>-1.437971</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.089046</td>\n",
       "      <td>-131.557953</td>\n",
       "      <td>-154.041611</td>\n",
       "      <td>-2.503690</td>\n",
       "      <td>-2.391870</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0459</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-2.095844</td>\n",
       "      <td>-2.092802</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>-0.003042</td>\n",
       "      <td>-148.134857</td>\n",
       "      <td>-136.043182</td>\n",
       "      <td>-2.208426</td>\n",
       "      <td>-2.257729</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.334</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-3.077222</td>\n",
       "      <td>-3.538402</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.461179</td>\n",
       "      <td>-143.582916</td>\n",
       "      <td>-118.4935</td>\n",
       "      <td>-2.307107</td>\n",
       "      <td>-2.416669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.114</td>\n",
       "      <td>0.514</td>\n",
       "      <td>-3.077222</td>\n",
       "      <td>-3.538402</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.461179</td>\n",
       "      <td>-143.582916</td>\n",
       "      <td>-118.4935</td>\n",
       "      <td>-2.307107</td>\n",
       "      <td>-2.416669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  learning_rate  rewards/chosen  rewards/rejected  \\\n",
       "0  1.0057          0.001       -1.083685         -0.877419   \n",
       "1  0.8486          0.001       -1.390192         -1.674966   \n",
       "2  0.9214          0.001       -1.527017         -1.437971   \n",
       "3  1.0459          0.001       -2.095844         -2.092802   \n",
       "4     NaN            NaN             NaN               NaN   \n",
       "5     NaN            NaN             NaN               NaN   \n",
       "6     NaN            NaN             NaN               NaN   \n",
       "\n",
       "   rewards/accuracies  rewards/margins  logps/rejected  logps/chosen  \\\n",
       "0            0.406250        -0.206266     -164.839951   -152.500244   \n",
       "1            0.578125         0.284774     -139.483093   -131.596252   \n",
       "2            0.500000        -0.089046     -131.557953   -154.041611   \n",
       "3            0.468750        -0.003042     -148.134857   -136.043182   \n",
       "4                 NaN              NaN             NaN           NaN   \n",
       "5                 NaN              NaN             NaN           NaN   \n",
       "6                 NaN              NaN             NaN           NaN   \n",
       "\n",
       "   logits/rejected  logits/chosen  ...  eval_samples_per_second  \\\n",
       "0        -2.065371      -2.007442  ...                      NaN   \n",
       "1        -2.024166      -1.993961  ...                      NaN   \n",
       "2        -2.503690      -2.391870  ...                      NaN   \n",
       "3        -2.208426      -2.257729  ...                      NaN   \n",
       "4              NaN            NaN  ...                      NaN   \n",
       "5              NaN            NaN  ...                    3.334   \n",
       "6              NaN            NaN  ...                    4.114   \n",
       "\n",
       "   eval_steps_per_second  eval_rewards/chosen  eval_rewards/rejected  \\\n",
       "0                    NaN                  NaN                    NaN   \n",
       "1                    NaN                  NaN                    NaN   \n",
       "2                    NaN                  NaN                    NaN   \n",
       "3                    NaN                  NaN                    NaN   \n",
       "4                    NaN                  NaN                    NaN   \n",
       "5                  0.417            -3.077222              -3.538402   \n",
       "6                  0.514            -3.077222              -3.538402   \n",
       "\n",
       "   eval_rewards/accuracies  eval_rewards/margins  eval_logps/rejected  \\\n",
       "0                      NaN                   NaN                  NaN   \n",
       "1                      NaN                   NaN                  NaN   \n",
       "2                      NaN                   NaN                  NaN   \n",
       "3                      NaN                   NaN                  NaN   \n",
       "4                      NaN                   NaN                  NaN   \n",
       "5                    0.514              0.461179          -143.582916   \n",
       "6                    0.514              0.461179          -143.582916   \n",
       "\n",
       "   eval_logps/chosen  eval_logits/rejected  eval_logits/chosen  \n",
       "0                NaN                   NaN                 NaN  \n",
       "1                NaN                   NaN                 NaN  \n",
       "2                NaN                   NaN                 NaN  \n",
       "3                NaN                   NaN                 NaN  \n",
       "4                NaN                   NaN                 NaN  \n",
       "5          -118.4935             -2.307107           -2.416669  \n",
       "6          -118.4935             -2.307107           -2.416669  \n",
       "\n",
       "[7 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# view as records\n",
    "\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use gpu for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some popular cities for tourists are New York, Las Vegas, Los Angeles, Chicago, Washington, D.C., and Boston. But many choose to travel to unique, smaller cities, and even to places very far from the coast, like Buffalo, Syracuse, Rochester, and Niagara Falls, in upstate New York. Many people are drawn to these cities because of their more walkable downtowns, their smaller sizes, and their greater diversity of people, shops and restaurants.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "prompt = 'Some popular cities for tourists are'\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "input_ids = input_ids.to(device)\n",
    "output_ids = model.generate(input_ids, max_length=128, do_sample=True, num_return_sequences=1)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
