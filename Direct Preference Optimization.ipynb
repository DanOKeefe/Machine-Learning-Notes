{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization (DPO)\n",
    "\n",
    "[Direct Preference Optimziation](https://arxiv.org/abs/2305.18290) is method for improving the alignment of language models.\n",
    "\n",
    "Current approaches for training large language models utilize an intial step of unsupervised training on a large corpus of data. This yields models that generate tokens most likely to follow prompts based on the conditional distribution of the data it was trained on. When we want our language model to be used as a chat bot or code assistant, this oftentimes produces undesirable text. High quality conversations or coding examples may be rare in our training corpus, thus rare in our model's output.\n",
    "\n",
    "To address this, we can utilize a dataset to have our model generate text more closely aligned with a downstream task. Our dataset will have three columns: a prompt, a chosen output, and a rejected output. Given the prompt, we want our model to generate an output closer to the chosen output than the rejected output. In this example we use a dataset where the preferred output is considered more safe and appropriate for a chat bot.\n",
    "\n",
    "How can we structure a this problem in a way that our model can learn from it?\n",
    "\n",
    "One approach is Reinforcement Learning with Human Feedback ([RLHF](https://arxiv.org/abs/2203.02155)). With RLHF, we first train a binary classifier that learns to discriminate between outputs. A Bradley-Terry model is often used as the classifier here, where the probability that output y1 is preferred to y2 is defined as p(y1 >> y2 | x) = sigmoid(r(x,y1)-r(x,y2)), where >> denotes `preferred to`. And r(x,y) is the reward. The reward function is learned using binary cross-entropy loss. As a starting point for our reward function, we can use a supervised and fine-tuned (SFT) LLM with the last layer changed to output a scalar value, representing the reward. The next step in RLHF is to use an algorithm like PPO to optimize our policy to maximize this reward while not straying too far from a reference policy. In this step, we sample prompts from our dataset and send them to our reward model to get a score. During this fine-tuning process, we aim to maximize our reward while mimizing the KL divergence between the token-level probability distribution output of our fine-tuned model and of the output of our reference model.\n",
    "\n",
    "A downside of this approach is that it is expensive to first train a reward model and then sample from it inside the training loop during the fine-tuning process. RLHF is also complex and often unstable.\n",
    "\n",
    "An alternative approach is DPO, which doesn't use reinforcement learning. From the DPO paper:\n",
    "\n",
    "```Instead of using the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, DPO uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data.```\n",
    "\n",
    "With DPO, the preference model is now expressed in terms of the optimal policy and reference policy instead of using the reward model. The loss function, as layed out in the paper, is changed accordingly.\n",
    "\n",
    "The [trl](https://github.com/huggingface/trl/) library has a DPOTrainer, subclassed from the Huggingface Trainer class. DPOTrainer has an implementation of the DPO loss function, as well as methods to help with tokenization of inputs, generating predictions, and evaluation. It also supports utilization of PEFT (Parameter-Efficient Fine-Tuning), which we will use to implement [LoRA](https://arxiv.org/abs/2106.09685) (Low-Rank Adaption of Large Language Models). This, along with quantization (utilizing 8-bit model weights), allows us to reduce the number of trainable parameters and memory footprint of the model so that we can fine-tune the model on a local machine with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import DPOTrainer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# note: to get bitsandbytes to work on windows, uninstall bitsandbytes and reinstall with\n",
    "# pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hh_rlhf_sample(sample):\n",
    "    \"\"\"\n",
    "    sample is a dictionary with keys 'chosen' and 'rejected'.\n",
    "    Extract the prompt and the two completions from the sample.\n",
    "    Find the index of the last substring '\\n\\nAssistant:\n",
    "\n",
    "    Return a dictionary with keys prompt, chosen, and rejected.\n",
    "    \"\"\"\n",
    "    term = '\\n\\nAssistant: '\n",
    "    end_of_prompt_index = sample['chosen'].rfind(term)\n",
    "\n",
    "    # extract the prompt\n",
    "    prompt = sample['chosen'][:end_of_prompt_index+len(term)]\n",
    "    # extract the chosen completion\n",
    "    chosen = sample['chosen'][len(prompt):]\n",
    "    # extract the rejected completion\n",
    "    rejected = sample['rejected'][len(prompt):]\n",
    "\n",
    "    return {'prompt': prompt, 'chosen': chosen, 'rejected': rejected}\n",
    "\n",
    "def get_anthropic_hh_rlhf_dataset(split='train'):\n",
    "    \"\"\"\n",
    "    The Anthropic HH-RLHF dataset contains 160k training examples and 8k test examples.\n",
    "    Each example is a dictionary with two keys: 'chosen' and 'rejected'.\n",
    "    Each of these includes the prompt and the completion.\n",
    "    I want to extract the prompt, chosen completion, and rejected completion.\n",
    "\n",
    "    https://arxiv.org/abs/2204.05862\n",
    "    https://huggingface.co/datasets/Anthropic/hh-rlhf\n",
    "    \"\"\"\n",
    "    dataset = load_dataset('Anthropic/hh-rlhf', split=split)\n",
    "    return dataset.map(process_hh_rlhf_sample)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'stabilityai/stablelm-2-1_6b',\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'stabilityai/stablelm-2-1_6b',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# https://github.com/huggingface/trl/issues/1073\n",
    "tokenizer.add_special_tokens({\"bos_token\": tokenizer.eos_token})\n",
    "tokenizer.bos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "train_dataset = get_anthropic_hh_rlhf_dataset(split='train')\n",
    "test_dataset = get_anthropic_hh_rlhf_dataset(split='test[:1000]') # use a small test set for now\n",
    "\n",
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=64, # only 64 gradient updates, not even one epoch\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    output_dir='output',\n",
    "    logging_strategy='steps',\n",
    "    logging_dir='logs',\n",
    "    logging_steps=16,\n",
    "    lr_scheduler_type='constant' # default is linear\n",
    ")\n",
    "\n",
    "# after training, I can view the logs with:\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64, # dimension of the low-rank matrices\n",
    "    lora_alpha=16, # scaling factor for the weight matrices\n",
    "    bias='none', # don't train bias params\n",
    "    task_type='CASUAL_LM',\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'o_proj',\n",
    "        'gate_proj',\n",
    "        'up_proj',\n",
    "        'down_proj',\n",
    "        'lm_head',\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples['prompt'], examples['chosen'], examples['rejected'], padding=True, truncation=True)\n",
    "\n",
    "# tokenize the datasets\n",
    "encoded_dataset_train = train_dataset.map(tokenize_func, batched=True)\n",
    "encoded_dataset_test = test_dataset.map(tokenize_func, batched=True)\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    beta=0.1, # beta param for DPO loss\n",
    "    train_dataset=encoded_dataset_train,\n",
    "    eval_dataset=encoded_dataset_test,\n",
    "    max_length=512, # max length of the input\n",
    "    max_target_length=128, #\n",
    "    max_prompt_length=128,\n",
    "    generate_during_eval=False,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model before training.\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and evaluate the model after training.\n",
    "trainer.save_model('out')\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training loss after logged steps.\n",
    "print(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use gpu for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off warnings for this cell\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.to_device()\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "prompt = 'Some popular cities for tourists are'\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "input_ids = input_ids.to(device)\n",
    "output_ids = model.generate(input_ids, max_length=128, do_sample=True, num_return_sequences=1)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
